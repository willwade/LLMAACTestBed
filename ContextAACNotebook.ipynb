{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context-Aware AAC Testbed: LLM Experimentation\n",
    "\n",
    "**Objective:** To determine if injecting specific contextual data (User Profile, Time, Location) into a Large Language Model (LLM) improves the accuracy and utility of phrase predictions for a user with Motor Neurone Disease (MND).\n",
    "\n",
    "## The Hypothesis\n",
    "\n",
    "Standard predictive text and generic LLMs fail AAC users because they prioritize \"polite conversation\" over \"functional tools.\" We hypothesize that by layering **Static Context** (User Profile) and **Dynamic Context** (Time/Location) over speech input, we can move from generic chat to precise intent prediction.\n",
    "\n",
    "**Subject Persona:** \"Dave\" – Late-stage MND, telegraphic speech, developer background.\n",
    "**Critical Constraints:** High fatigue (limited breath for speech), temperature dysregulation, dependence on specific equipment (NIV Mask, Fan)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Installation\n",
    "First, we install the necessary libraries. We are using Simon Willison's `llm` library for easy model interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install llm llm-gemini pandas matplotlib seaborn\n",
    "\n",
    "import os\n",
    "import json\n",
    "import llm\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from google.colab import userdata\n",
    "\n",
    "# --- API KEY SETUP ---\n",
    "# You need a Google AI Studio Key.\n",
    "# Option 1: Set it in Colab Secrets (Key name: GOOGLE_API_KEY)\n",
    "# Option 2: Uncomment the input line below to paste it manually\n",
    "\n",
    "try:\n",
    "    os.environ[\"GOOGLE_API_KEY\"] = userdata.get('GOOGLE_API_KEY')\n",
    "except:\n",
    "    print(\"Colab Secret not found. Please paste your key below.\")\n",
    "    os.environ[\"GOOGLE_API_KEY\"] = input(\"Enter Google API Key: \")\n",
    "\n",
    "# Configure llm to use the gemini model\n",
    "# (The plugin auto-detects the env var, but we verify functionality)\n",
    "model = llm.get_model(\"gemini-1.5-flash\") \n",
    "print(f\"Setup Complete. Using model: {model.model_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Local Data\n",
    "**IMPORTANT:** Please ensure you have uploaded the following files to the Colab \"Files\" tab on the left:\n",
    "1. `dave_context.json`\n",
    "2. `transcript_data_2.json`\n",
    "3. `transcript_vague.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Static Profile (The Knowledge Graph)\n",
    "try:\n",
    "    with open('dave_context.json', 'r') as f:\n",
    "        dave_profile = json.load(f)\n",
    "    print(\"✅ Dave's Profile Loaded.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"❌ ERROR: 'dave_context.json' not found. Please upload it to the Files tab.\")\n",
    "\n",
    "# Load the Transcripts (The Scenarios)\n",
    "try:\n",
    "    with open('transcript_data_2.json', 'r') as f:\n",
    "        transcript_strict = json.load(f)\n",
    "    with open('transcript_vague.json', 'r') as f:\n",
    "        transcript_vague = json.load(f)\n",
    "    print(\"✅ Transcripts Loaded.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"❌ ERROR: Transcript files not found. Please upload 'transcript_data_2.json' and 'transcript_vague.json'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Experiment 1: The Baseline (Context vs. Noise)\n",
    "\n",
    "**Hypothesis:** Providing environmental data (Time, Location, People) without speech will result in hallucinations, while Speech + Profile will provide a strong baseline.\n",
    "\n",
    "We compare 5 hypotheses:\n",
    "* **H1:** Time Only\n",
    "* **H2:** Time + Who\n",
    "* **H3:** Time + Who + Location\n",
    "* **H5:** Speech + Profile (Baseline)\n",
    "* **H4:** Full Context (Speech + Profile + Time + Who + Location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CONFIGURATION ---\n",
    "MODEL_NAME = \"gemini-1.5-flash\" \n",
    "\n",
    "def generate_prediction(model, system_prompt, user_prompt):\n",
    "    try:\n",
    "        # Low temp for consistency\n",
    "        response = model.prompt(user_prompt, system=system_prompt, temperature=0.2)\n",
    "        text = response.text().strip()\n",
    "        return text.replace(\"\\n\", \" \")\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "def evaluate_intent_match(model, target, prediction):\n",
    "    \"\"\"\n",
    "    Judges the semantic closeness of the prediction to the target (1-10).\n",
    "    \"\"\"\n",
    "    judge_system = \"You are a semantic evaluator for an AAC system.\"\n",
    "    judge_prompt = f\"\"\"\n",
    "    Compare these two phrases.\n",
    "    1. TARGET INTENT: \"{target}\"\n",
    "    2. AI PREDICTION: \"{prediction}\"\n",
    "    \n",
    "    Rate the similarity of the INTENT (Actionability/Meaning) on a scale of 1 to 10.\n",
    "    1 = Completely wrong/harmful.\n",
    "    5 = Vague or related topic but wrong action.\n",
    "    10 = Perfect match.\n",
    "    \n",
    "    Return ONLY the integer.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = model.prompt(judge_prompt, system=judge_system, temperature=0.0)\n",
    "        score = \"\".join(filter(str.isdigit, response.text()))\n",
    "        return int(score) if score else 0\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def run_strict_experiment():\n",
    "    if 'dave_profile' not in globals() or 'transcript_strict' not in globals():\n",
    "        print(\"❌ Data not loaded. Cannot run experiment.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    model = llm.get_model(MODEL_NAME)\n",
    "    \n",
    "    base_system_prompt = f\"\"\"\n",
    "    You are a Predictive AAC System for a user named Dave.\n",
    "    USER PROFILE:\n",
    "    {json.dumps(dave_profile, indent=2)}\n",
    "    INSTRUCTIONS:\n",
    "    1. Predict the most likely short phrase Dave wants to say.\n",
    "    2. Base prediction ONLY on the INPUT DATA provided.\n",
    "    3. Output ONLY the predicted phrase.\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"Running Full Spectrum Experiment...\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for turn in transcript_strict:\n",
    "        print(f\"Processing ID {turn['id']} ({turn['target_ground_truth']})...\")\n",
    "        \n",
    "        # Data Points\n",
    "        time = turn[\"metadata\"][\"time\"]\n",
    "        participants = \", \".join(turn[\"metadata\"][\"active_participants\"])\n",
    "        location = turn[\"metadata\"][\"location\"]\n",
    "        prev_utterance = f\"Previous Speaker said: '{turn['dialogue_history']['last_utterance']}'\"\n",
    "        \n",
    "        # --- H1: TIME ONLY ---\n",
    "        h1_pred = generate_prediction(model, base_system_prompt, f\"INPUT: Time: {time}\")\n",
    "        h1_score = evaluate_intent_match(model, turn[\"target_ground_truth\"], h1_pred)\n",
    "\n",
    "        # --- H2: TIME + WHO ---\n",
    "        h2_pred = generate_prediction(model, base_system_prompt, f\"INPUT: Time: {time}. People: {participants}\")\n",
    "        h2_score = evaluate_intent_match(model, turn[\"target_ground_truth\"], h2_pred)\n",
    "\n",
    "        # --- H3: TIME + WHO + LOC ---\n",
    "        h3_pred = generate_prediction(model, base_system_prompt, f\"INPUT: Time: {time}. People: {participants}. Location: {location}\")\n",
    "        h3_score = evaluate_intent_match(model, turn[\"target_ground_truth\"], h3_pred)\n",
    "\n",
    "        # --- H5: SPEECH ONLY (No environmental context) ---\n",
    "        h5_pred = generate_prediction(model, base_system_prompt, f\"INPUT: {prev_utterance}\")\n",
    "        h5_score = evaluate_intent_match(model, turn[\"target_ground_truth\"], h5_pred)\n",
    "\n",
    "        # --- H4: FULL CONTEXT (All inputs) ---\n",
    "        h4_pred = generate_prediction(model, base_system_prompt, f\"INPUT: Time: {time}. People: {participants}. Location: {location}. {prev_utterance}\")\n",
    "        h4_score = evaluate_intent_match(model, turn[\"target_ground_truth\"], h4_pred)\n",
    "\n",
    "        results.append({\n",
    "            'ID': turn['id'],\n",
    "            'Target': turn['target_ground_truth'],\n",
    "            'H1_Time': h1_pred, 'H1_Score': h1_score,\n",
    "            'H2_Who': h2_pred, 'H2_Score': h2_score,\n",
    "            'H3_Loc': h3_pred, 'H3_Score': h3_score,\n",
    "            'H5_Speech': h5_pred, 'H5_Score': h5_score,\n",
    "            'H4_Full': h4_pred, 'H4_Score': h4_score\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "df_results = run_strict_experiment()\n",
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualizing the Context Advantage\n",
    "We compare **H5 (Speech + Profile)** vs **H4 (Full Context)** to see where context closes the gap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df_results.empty:\n",
    "    # Setup the plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.set_style(\"whitegrid\")\n",
    "\n",
    "    # Melt dataframe for Seaborn\n",
    "    df_melted = df_results.melt(id_vars=['ID', 'Target'], \n",
    "                                value_vars=['H5_Score', 'H4_Score'], \n",
    "                                var_name='Hypothesis', \n",
    "                                value_name='Accuracy_Score')\n",
    "\n",
    "    # Custom colors: Grey for Speech Only, Green for Context\n",
    "    palette = {\"H5_Score\": \"#95a5a6\", \"H4_Score\": \"#2ecc71\"}\n",
    "\n",
    "    ax = sns.barplot(x='ID', y='Accuracy_Score', hue='Hypothesis', data=df_melted, palette=palette)\n",
    "\n",
    "    plt.title('Impact of Context Awareness on AAC Prediction Accuracy', fontsize=16)\n",
    "    plt.xlabel('Scenario ID', fontsize=12)\n",
    "    plt.ylabel('Semantic Accuracy Score (1-10)', fontsize=12)\n",
    "    plt.ylim(0, 11)\n",
    "    plt.legend(title='Model Configuration', labels=['H5: Speech Only', 'H4: Full Context'])\n",
    "\n",
    "    # Annotate the bars with the target intent\n",
    "    for i in range(len(df_results)):\n",
    "        row = df_results.iloc[i]\n",
    "        plt.text(i, 10.2, row['Target'], ha='center', fontsize=9, rotation=0, color='#34495e')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No results to plot.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Experiment 2: The Ablation Test (Value of Profile)\n",
    "\n",
    "**Hypothesis:** If we strip away the User Profile (The Knowledge Graph) and rely on \"Raw Speech\" (like a standard chatbot), the utility of the system will collapse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ablation_experiment():\n",
    "    if 'dave_profile' not in globals() or 'transcript_vague' not in globals():\n",
    "        print(\"❌ Data not loaded. Cannot run experiment.\")\n",
    "        return\n",
    "\n",
    "    model = llm.get_model(MODEL_NAME)\n",
    "\n",
    "    # SYSTEM PROMPT 1: DAVE-AWARE\n",
    "    smart_system = f\"\"\"\n",
    "    You are an AAC assistant for Dave.\n",
    "    PROFILE: {json.dumps(dave_profile)}\n",
    "    Predict his response based on the input speech. Short phrases only.\n",
    "    \"\"\"\n",
    "\n",
    "    # SYSTEM PROMPT 2: GENERIC\n",
    "    dumb_system = \"\"\"\n",
    "    You are a helpful predictive text assistant. \n",
    "    Predict the next logical response based on the input speech. \n",
    "    Keep it short.\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"Running 'Speech Only' Ablation Test on {MODEL_NAME}\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"{'INPUT SPEECH':<40} | {'SMART (Profile)':<20} | {'RAW (No Profile)'}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    for turn in transcript_vague:\n",
    "        user_input = f\"Previous speaker said: '{turn['last_utterance']}'\"\n",
    "\n",
    "        # 1. Run Smart\n",
    "        smart_pred = model.prompt(user_input, system=smart_system, temperature=0.1).text().strip()\n",
    "        \n",
    "        # 2. Run Raw\n",
    "        raw_pred = model.prompt(user_input, system=dumb_system, temperature=0.1).text().strip()\n",
    "        \n",
    "        # Output\n",
    "        print(f\"'{turn['last_utterance'][:35]:<38}' | {smart_pred:<20} | {raw_pred}\")\n",
    "\n",
    "run_ablation_experiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Experiment 3: The Synthesis (Value of Time)\n",
    "\n",
    "**Hypothesis:** Time is the \"Key\" that unlocks ambiguous speech variables (e.g., \"The Usual\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_synthesis_experiment():\n",
    "    if 'dave_profile' not in globals():\n",
    "        print(\"❌ Data not loaded. Cannot run experiment.\")\n",
    "        return\n",
    "\n",
    "    model = llm.get_model(MODEL_NAME)\n",
    "\n",
    "    scenarios = [\n",
    "        {\n",
    "            \"speech\": \"Do you want the usual?\",\n",
    "            \"time\": \"08:00\", \n",
    "            \"context_note\": \"Morning\"\n",
    "        },\n",
    "        {\n",
    "            \"speech\": \"Do you want the usual?\",\n",
    "            \"time\": \"20:00\", \n",
    "            \"context_note\": \"Evening\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    print(f\"Running Temporal Context Test on {MODEL_NAME}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    for sc in scenarios:\n",
    "        current_system = f\"\"\"\n",
    "        You are an AAC assistant for Dave.\n",
    "        \n",
    "        USER PROFILE:\n",
    "        {json.dumps(dave_profile, indent=2)}\n",
    "        \n",
    "        CURRENT CONTEXT:\n",
    "        Time: {sc['time']}\n",
    "        \n",
    "        INSTRUCTIONS:\n",
    "        Predict Dave's response based on the input speech. \n",
    "        If the speech is vague (e.g. \"the usual\"), use the TIME and the PROFILE to guess the routine.\n",
    "        Output only the short phrase response.\n",
    "        \"\"\"\n",
    "        \n",
    "        input_text = f\"Kelsey said: '{sc['speech']}'\"\n",
    "        \n",
    "        # Run prediction\n",
    "        prediction = model.prompt(input_text, system=current_system, temperature=0.1).text().strip()\n",
    "        \n",
    "        print(f\"Time: {sc['time']} ({sc['context_note']})\")\n",
    "        print(f\"Input: '{sc['speech']}'\")\n",
    "        print(f\"Prediction: {prediction}\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "run_synthesis_experiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "1.  **Raw LLMs are insufficient for AAC:** They default to polite conversation (\"I feel warm\") rather than actionable commands (\"Fan on\").\n",
    "2.  **The Static Profile is the Foundation:** Providing the LLM with a JSON structure of medical needs and habits solves 80% of prediction issues.\n",
    "3.  **Time is the Disambiguator:** For vague shorthand (\"The usual\", \"Not now\"), Time is the critical variable that prevents hallucination.\n",
    "4.  **The \"Deictic Wall\":** Text-based LLMs cannot solve \"Do you want *this*?\" references. This requires Multimodal inputs (Vision/Camera)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}