{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Chat History-Driven LLM Evaluation\n",
        "\n",
        "**Objective:** To evaluate LLM-based text completion systems for AAC users using real chat history data. This framework allows us to compare different approaches for generating message proposals based on partial utterances.\n",
        "\n",
        "## Overview\n",
        "\n",
        "This evaluation framework:\n",
        "1. Splits data chronologically into corpus (for conditioning) and test sets\n",
        "2. Generates proposals for partial test utterances using different methods\n",
        "3. Scores proposals against the true full utterances using multiple metrics\n",
        "\n",
        "The framework is designed to be extensible, supporting:\n",
        "- Multiple partial-utterance constructions (prefix truncation, keyword subsets)\n",
        "- Multiple proposal-generation methods (RAG, semantic search)\n",
        "- Multiple evaluation metrics (embedding similarity, LLM-judge scoring)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies (run once)\n",
        "# !pip install llm llm-gemini pandas matplotlib seaborn sentence-transformers scikit-learn tenacity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "import re\n",
        "from typing import List, Dict, Tuple, Callable\n",
        "from collections import Counter\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# LLM and embedding libraries\n",
        "import llm\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type\n",
        "from getpass import getpass\n",
        "\n",
        "# --- API KEY SETUP ---\n",
        "if \"LLM_GEMINI_KEY\" not in os.environ:\n",
        "    os.environ[\"LLM_GEMINI_KEY\"] = getpass(\"Enter your Gemini API Key: \")\n",
        "\n",
        "# Configure models\n",
        "GENERATIVE_MODEL = llm.get_model(\"gemini-2.0-flash-exp\")\n",
        "JUDGE_MODEL = llm.get_model(\"gemini-2.0-flash-exp\")\n",
        "EMBEDDING_MODEL = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "print(\"✅ Setup Complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Data Loading and Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the chat history data\n",
        "# IMPORTANT: This file contains real user speech history and should not be committed to GitHub\n",
        "try:\n",
        "    with open('../baton-export-2025-11-24-nofullstop.json', 'r') as f:\n",
        "        chat_data = json.load(f)\n",
        "    print(f\"✅ Loaded {len(chat_data['sentences'])} utterances from chat history.\")\n",
        "except FileNotFoundError:\n",
        "    print(\"❌ ERROR: Chat history file not found. Make sure the path is correct.\")\n",
        "    print(\"Expected path: ../baton-export-2025-11-24-nofullstop.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert to DataFrame for easier manipulation\n",
        "def preprocess_chat_data(chat_data):\n",
        "    \"\"\"\n",
        "    Convert the raw chat data into a pandas DataFrame with additional features.\n",
        "    \"\"\"\n",
        "    sentences = chat_data['sentences']\n",
        "    \n",
        "    # Extract relevant fields\n",
        "    processed_data = []\n",
        "    \n",
        "    for sentence in sentences:\n",
        "        # Get the primary content and metadata\n",
        "        content = sentence['content']\n",
        "        \n",
        "        # Extract timestamp from metadata\n",
        "        if sentence.get('metadata') and len(sentence['metadata']) > 0:\n",
        "            # Some entries have multiple metadata entries, use the first one with a timestamp\n",
        "            timestamp_str = None\n",
        "            latitude = longitude = None\n",
        "            \n",
        "            for meta in sentence['metadata']:\n",
        "                if 'timestamp' in meta:\n",
        "                    timestamp_str = meta['timestamp']\n",
        "                    latitude = meta.get('latitude')\n",
        "                    longitude = meta.get('longitude')\n",
        "                    break\n",
        "                    \n",
        "            # Convert timestamp to datetime\n",
        "            if timestamp_str:\n",
        "                try:\n",
        "                    # Try parsing with timezone\n",
        "                    timestamp = datetime.fromisoformat(timestamp_str.replace('Z', '+00:00'))\n",
        "                except ValueError:\n",
        "                    # Fallback for different formats\n",
        "                    timestamp = datetime.strptime(timestamp_str, '%Y-%m-%dT%H:%M:%S.%fZ')\n",
        "            else:\n",
        "                timestamp = None\n",
        "        else:\n",
        "            timestamp = latitude = longitude = None\n",
        "        \n",
        "        # Calculate utterance length in words\n",
        "        word_count = len(content.split()) if content else 0\n",
        "        \n",
        "        processed_data.append({\n",
        "            'content': content,\n",
        "            'timestamp': timestamp,\n",
        "            'latitude': latitude,\n",
        "            'longitude': longitude,\n",
        "            'word_count': word_count,\n",
        "            'uuid': sentence.get('uuid'),\n",
        "            'anonymous_uuid': sentence.get('anonymousUUID')\n",
        "        })\n",
        "    \n",
        "    df = pd.DataFrame(processed_data)\n",
        "    \n",
        "    # Drop rows with missing content or timestamp\n",
        "    df = df.dropna(subset=['content', 'timestamp'])\n",
        "    \n",
        "    # Sort by timestamp (chronological order)\n",
        "    df = df.sort_values('timestamp').reset_index(drop=True)\n",
        "    \n",
        "    # Add time-of-day features\n",
        "    df['hour'] = df['timestamp'].dt.hour\n",
        "    df['day_of_week'] = df['timestamp'].dt.dayofweek\n",
        "    \n",
        "    return df\n",
        "\n",
        "# Process the data\n",
        "chat_df = preprocess_chat_data(chat_data)\n",
        "print(f\"✅ Processed data: {len(chat_df)} utterances with timestamps.\")\n",
        "print(f\"Date range: {chat_df['timestamp'].min()} to {chat_df['timestamp'].max()}\")\n",
        "print(f\"Average utterance length: {chat_df['word_count'].mean():.1f} words\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Splitting the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split the data chronologically\n",
        "def split_data(df, corpus_ratio=0.67):\n",
        "    \"\"\"\n",
        "    Split the data into corpus (for conditioning) and test sets chronologically.\n",
        "    \n",
        "    Args:\n",
        "        df: DataFrame with chat data\n",
        "        corpus_ratio: Fraction of data to use as corpus (default: 0.67 = 2/3)\n",
        "    \n",
        "    Returns:\n",
        "        corpus_df: First portion of data for retrieval/RAG\n",
        "        test_df: Final portion of data for evaluation\n",
        "    \"\"\"\n",
        "    split_idx = int(len(df) * corpus_ratio)\n",
        "    \n",
        "    corpus_df = df.iloc[:split_idx].reset_index(drop=True)\n",
        "    test_df = df.iloc[split_idx:].reset_index(drop=True)\n",
        "    \n",
        "    return corpus_df, test_df\n",
        "\n",
        "# Split the data\n",
        "corpus_df, test_df = split_data(chat_df)\n",
        "print(f\"Corpus (for conditioning): {len(corpus_df)} utterances\")\n",
        "print(f\"Test (for evaluation): {len(test_df)} utterances\")\n",
        "\n",
        "# Visualize the split\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(corpus_df['timestamp'], range(len(corpus_df)), 'b-', label='Corpus')\n",
        "plt.plot(test_df['timestamp'], range(len(test_df)), 'r-', label='Test')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Utterance Index')\n",
        "plt.title('Chronological Distribution of Data')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.pie([len(corpus_df), len(test_df)], labels=['Corpus', 'Test'], autopct='%1.1f%%')\n",
        "plt.title('Data Split Ratio')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Partial Utterance Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Different methods to create partial utterances\n",
        "def create_prefix_partial(text, n_words=3):\n",
        "    \"\"\"\n",
        "    Create a partial utterance by taking the first N words.\n",
        "    \n",
        "    Args:\n",
        "        text: Original full utterance\n",
        "        n_words: Number of words to include in the partial utterance\n",
        "    \n",
        "    Returns:\n",
        "        Partial utterance consisting of the first N words\n",
        "    \"\"\"\n",
        "    words = text.split()\n",
        "    if len(words) <= n_words:\n",
        "        return text\n",
        "    return ' '.join(words[:n_words])\n",
        "\n",
        "def create_keyword_partial(text, n_keywords=2):\n",
        "    \"\"\"\n",
        "    Create a partial utterance by extracting the most salient keywords.\n",
        "    Uses TF-IDF to identify important words.\n",
        "    \n",
        "    Args:\n",
        "        text: Original full utterance\n",
        "        n_keywords: Number of keywords to include in the partial utterance\n",
        "    \n",
        "    Returns:\n",
        "        Partial utterance consisting of the most important keywords\n",
        "    \"\"\"\n",
        "    words = text.split()\n",
        "    if len(words) <= n_keywords:\n",
        "        return text\n",
        "        \n",
        "    # Simple keyword extraction using word frequency\n",
        "    word_freq = Counter([word.lower() for word in words if len(word) > 2])\n",
        "    \n",
        "    # Get the most frequent words\n",
        "    top_words = [word for word, count in word_freq.most_common(n_keywords)]\n",
        "    \n",
        "    # Find these words in the original order\n",
        "    partial_words = []\n",
        "    for word in words:\n",
        "        if word.lower() in top_words and len(partial_words) < n_keywords:\n",
        "            partial_words.append(word)\n",
        "    \n",
        "    # If we couldn't find enough unique words, fall back to first N words\n",
        "    if len(partial_words) < n_keywords:\n",
        "        return create_prefix_partial(text, n_keywords)\n",
        "        \n",
        "    return ' '.join(partial_words)\n",
        "\n",
        "def create_random_partial(text, min_words=1, max_words=3):\n",
        "    \"\"\"\n",
        "    Create a partial utterance by selecting random words.\n",
        "    \n",
        "    Args:\n",
        "        text: Original full utterance\n",
        "        min_words: Minimum number of words to include\n",
        "        max_words: Maximum number of words to include\n",
        "    \n",
        "    Returns:\n",
        "        Partial utterance with randomly selected words\n",
        "    \"\"\"\n",
        "    words = text.split()\n",
        "    n_words = min(len(words), np.random.randint(min_words, max_words + 1))\n",
        "    \n",
        "    # Select random indices\n",
        "    indices = np.random.choice(len(words), size=n_words, replace=False)\n",
        "    indices = sorted(indices)  # Maintain original order\n",
        "    \n",
        "    return ' '.join([words[i] for i in indices])\n",
        "\n",
        "# Test the partial utterance functions\n",
        "test_utterance = \"I need to adjust my neck brace because it's uncomfortable\"\n",
        "print(f\"Original: {test_utterance}\")\n",
        "print(f\"Prefix (3 words): {create_prefix_partial(test_utterance, 3)}\")\n",
        "print(f\"Keyword (2 words): {create_keyword_partial(test_utterance, 2)}\")\n",
        "print(f\"Random (1-3 words): {create_random_partial(test_utterance)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Corpus-Based Retrieval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Functions for retrieving relevant examples from the corpus\n",
        "def retrieve_lexical_examples(corpus_df, partial_text, top_k=3):\n",
        "    \"\"\"\n",
        "    Retrieve corpus examples containing exact matches to the partial text.\n",
        "    \n",
        "    Args:\n",
        "        corpus_df: DataFrame with corpus utterances\n",
        "        partial_text: Partial utterance to search for\n",
        "        top_k: Maximum number of examples to retrieve\n",
        "    \n",
        "    Returns:\n",
        "        List of matching corpus utterances\n",
        "    \"\"\"\n",
        "    matching_examples = []\n",
        "    partial_words = set(partial_text.lower().split())\n",
        "    \n",
        "    for _, row in corpus_df.iterrows():\n",
        "        content = row['content']\n",
        "        content_words = set(content.lower().split())\n",
        "        \n",
        "        # Check if there's any overlap\n",
        "        overlap = len(partial_words.intersection(content_words))\n",
        "        \n",
        "        if overlap > 0:\n",
        "            matching_examples.append({\n",
        "                'content': content,\n",
        "                'overlap': overlap,\n",
        "                'timestamp': row['timestamp']\n",
        "            })\n",
        "    \n",
        "    # Sort by overlap count and take top_k\n",
        "    matching_examples.sort(key=lambda x: x['overlap'], reverse=True)\n",
        "    return matching_examples[:top_k]\n",
        "\n",
        "def retrieve_tfidf_examples(corpus_df, partial_text, top_k=3):\n",
        "    \"\"\"\n",
        "    Retrieve corpus examples using TF-IDF similarity.\n",
        "    \n",
        "    Args:\n",
        "        corpus_df: DataFrame with corpus utterances\n",
        "        partial_text: Partial utterance to search for\n",
        "        top_k: Maximum number of examples to retrieve\n",
        "    \n",
        "    Returns:\n",
        "        List of similar corpus utterances\n",
        "    \"\"\"\n",
        "    # Create TF-IDF vectorizer\n",
        "    corpus_contents = corpus_df['content'].tolist()\n",
        "    \n",
        "    # Add partial_text to the corpus for vectorization\n",
        "    all_texts = corpus_contents + [partial_text]\n",
        "    \n",
        "    vectorizer = TfidfVectorizer()\n",
        "    tfidf_matrix = vectorizer.fit_transform(all_texts)\n",
        "    \n",
        "    # Calculate similarity between partial_text and all corpus utterances\n",
        "    partial_vector = tfidf_matrix[-1]  # Last item is our partial text\n",
        "    corpus_vectors = tfidf_matrix[:-1]  # All but last are corpus items\n",
        "    \n",
        "    # Calculate cosine similarity\n",
        "    similarities = cosine_similarity(partial_vector, corpus_vectors).flatten()\n",
        "    \n",
        "    # Get the top_k most similar examples\n",
        "    top_indices = similarities.argsort()[-top_k:][::-1]\n",
        "    \n",
        "    examples = []\n",
        "    for idx in top_indices:\n",
        "        examples.append({\n",
        "            'content': corpus_contents[idx],\n",
        "            'similarity': similarities[idx],\n",
        "            'timestamp': corpus_df.iloc[idx]['timestamp']\n",
        "        })\n",
        "    \n",
        "    return examples\n",
        "\n",
        "def retrieve_embedding_examples(corpus_df, partial_text, top_k=3):\n",
        "    \"\"\"\n",
        "    Retrieve corpus examples using embedding similarity.\n",
        "    \n",
        "    Args:\n",
        "        corpus_df: DataFrame with corpus utterances\n",
        "        partial_text: Partial utterance to search for\n",
        "        top_k: Maximum number of examples to retrieve\n",
        "    \n",
        "    Returns:\n",
        "        List of similar corpus utterances\n",
        "    \"\"\"\n",
        "    # Generate embeddings for all corpus utterances\n",
        "    corpus_contents = corpus_df['content'].tolist()\n",
        "    corpus_embeddings = EMBEDDING_MODEL.encode(corpus_contents)\n",
        "    \n",
        "    # Generate embedding for partial text\n",
        "    partial_embedding = EMBEDDING_MODEL.encode([partial_text])\n",
        "    \n",
        "    # Calculate similarity\n",
        "    similarities = cosine_similarity(partial_embedding, corpus_embeddings).flatten()\n",
        "    \n",
        "    # Get the top_k most similar examples\n",
        "    top_indices = similarities.argsort()[-top_k:][::-1]\n",
        "    \n",
        "    examples = []\n",
        "    for idx in top_indices:\n",
        "        examples.append({\n",
        "            'content': corpus_contents[idx],\n",
        "            'similarity': similarities[idx],\n",
        "            'timestamp': corpus_df.iloc[idx]['timestamp']\n",
        "        })\n",
        "    \n",
        "    return examples\n",
        "\n",
        "# Test the retrieval functions\n",
        "test_partial = \"need to adjust\"\n",
        "print(f\"Testing retrieval with: '{test_partial}'\\n\")\n",
        "\n",
        "print(\"Lexical Retrieval:\")\n",
        "lexical_examples = retrieve_lexical_examples(corpus_df, test_partial)\n",
        "for example in lexical_examples:\n",
        "    print(f\"  {example['content']} (overlap: {example['overlap']})\")\n",
        "\n",
        "print(\"\\nTF-IDF Retrieval:\")\n",
        "tfidf_examples = retrieve_tfidf_examples(corpus_df, test_partial)\n",
        "for example in tfidf_examples:\n",
        "    print(f\"  {example['content']} (similarity: {example['similarity']:.3f})\")\n",
        "\n",
        "print(\"\\nEmbedding Retrieval:\")\n",
        "embedding_examples = retrieve_embedding_examples(corpus_df, test_partial)\n",
        "for example in embedding_examples:\n",
        "    print(f\"  {example['content']} (similarity: {example['similarity']:.3f})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Proposal Generation Methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define retry logic for LLM calls\n",
        "@retry(stop=stop_after_attempt(5), wait=wait_exponential(multiplier=1, min=4, max=10))\n",
        "def generate_completion(partial_text, examples=None, context=None):\n",
        "    \"\"\"\n",
        "    Generate a completion for a partial utterance using an LLM.\n",
        "    \n",
        "    Args:\n",
        "        partial_text: The partial utterance to complete\n",
        "        examples: List of retrieved examples from the corpus\n",
        "        context: Additional context (time, location, etc.)\n",
        "    \n",
        "    Returns:\n",
        "        Generated completion\n",
        "    \"\"\"\n",
        "    system_prompt = \"You are an intelligent AAC text completion system. Complete the user's partial text based on the provided context and examples.\"\n",
        "    \n",
        "    user_prompt = f\"Complete the following partial text: '{partial_text}'\\n\\n\"\n",
        "    \n",
        "    # Add context if provided\n",
        "    if context:\n",
        "        user_prompt += f\"Context: {context}\\n\\n\"\n",
        "    \n",
        "    # Add examples if provided\n",
        "    if examples:\n",
        "        user_prompt += \"Here are some examples of similar utterances:\\n\"\n",
        "        for i, example in enumerate(examples, 1):\n",
        "            user_prompt += f\"{i}. {example['content']}\\n\"\n",
        "        user_prompt += \"\\n\"\n",
        "    \n",
        "    user_prompt += \"Provide a completion that matches the user's likely intent. Only return the completed text, no explanation.\"\n",
        "    \n",
        "    try:\n",
        "        response = GENERATIVE_MODEL.prompt(user_prompt, system=system_prompt, temperature=0.2)\n",
        "        return response.text().strip()\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating completion: {e}\")\n",
        "        return partial_text  # Fallback to returning the partial text"
      ],
      "metadata": {
        "tags": []
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_with_lexical_retrieval(corpus_df, partial_text, context=None):\n",
        "    \"\"\"\n",
        "    Generate a completion using lexical retrieval examples.\n",
        "    \"\"\"\n",
        "    examples = retrieve_lexical_examples(corpus_df, partial_text)\n",
        "    return generate_completion(partial_text, examples, context)\n",
        "\n",
        "def generate_with_tfidf_retrieval(corpus_df, partial_text, context=None):\n",
        "    \"\"\"\n",
        "    Generate a completion using TF-IDF retrieval examples.\n",
        "    \"\"\"\n",
        "    examples = retrieve_tfidf_examples(corpus_df, partial_text)\n",
        "    return generate_completion(partial_text, examples, context)\n",
        "\n",
        "def generate_with_embedding_retrieval(corpus_df, partial_text, context=None):\n",
        "    \"\"\"\n",
        "    Generate a completion using embedding-based retrieval examples.\n",
        "    \"\"\"\n",
        "    examples = retrieve_embedding_examples(corpus_df, partial_text)\n",
        "    return generate_completion(partial_text, examples, context)\n",
        "\n",
        "def generate_with_context_only(partial_text, context):\n",
        "    \"\"\"\n",
        "    Generate a completion using only contextual information (no corpus examples).\n",
        "    \"\"\"\n",
        "    return generate_completion(partial_text, examples=None, context=context)\n",
        "\n",
        "# Test the proposal generation methods\n",
        "test_partial = \"need to adjust\"\n",
        "test_context = \"Time: 14:30, Location: Home\"\n",
        "\n",
        "print(f\"Testing proposal generation with: '{test_partial}'\\n\")\n",
        "\n",
        "print(\"With Lexical Retrieval:\")\n",
        "print(f\"  {generate_with_lexical_retrieval(corpus_df, test_partial, test_context)}\\n\")\n",
        "\n",
        "print(\"With TF-IDF Retrieval:\")\n",
        "print(f\"  {generate_with_tfidf_retrieval(corpus_df, test_partial, test_context)}\\n\")\n",
        "\n",
        "print(\"With Embedding Retrieval:\")\n",
        "print(f\"  {generate_with_embedding_retrieval(corpus_df, test_partial, test_context)}\\n\")\n",
        "\n",
        "print(\"With Context Only:\")\n",
        "print(f\"  {generate_with_context_only(test_partial, test_context)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Evaluation Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Functions to evaluate the quality of generated proposals\n",
        "def calculate_embedding_similarity(text1, text2):\n",
        "    \"\"\"\n",
        "    Calculate semantic similarity between two texts using embeddings.\n",
        "    \n",
        "    Args:\n",
        "        text1: First text\n",
        "        text2: Second text\n",
        "    \n",
        "    Returns:\n",
        "        Cosine similarity score between 0 and 1\n",
        "    \"\"\"\n",
        "    embeddings = EMBEDDING_MODEL.encode([text1, text2])\n",
        "    similarity = cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]\n",
        "    return float(similarity)\n",
        "\n",
        "# Define retry logic for LLM-based evaluation\n",
        "@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=2, max=5))\n",
        "def judge_similarity(target, proposal):\n",
        "    \"\"\"\n",
        "    Use an LLM to judge the semantic similarity between a target and proposal.\n",
        "    \n",
        "    Args:\n",
        "        target: The ground truth utterance\n",
        "        proposal: The generated proposal\n",
        "    \n",
        "    Returns:\n",
        "        Similarity score from 1 to 10\n",
        "    \"\"\"\n",
        "    system_prompt = \"You are an expert evaluator of AAC text completions.\"\n",
        "    \n",
        "    user_prompt = f\"\"\"\n",
        "    Compare these two phrases:\n",
        "    1. TARGET (what the user actually said): \"{target}\"\n",
        "    2. PROPOSAL (what the system predicted): \"{proposal}\"\n",
        "    \n",
        "    Rate the semantic similarity on a scale of 1 to 10:\n",
        "    1 = Completely unrelated or harmful\n",
        "    5 = Vaguely related but incorrect\n",
        "    10 = Perfect match in meaning and intent\n",
        "    \n",
        "    Return ONLY the integer score, nothing else.\n",
        "    \"\"\"\n",
        "    \n",
        "    try:\n",
        "        response = JUDGE_MODEL.prompt(user_prompt, system=system_prompt, temperature=0.0)\n",
        "        # Extract the integer score from the response\n",
        "        score_text = response.text().strip()\n",
        "        score = \"\".join(filter(str.isdigit, score_text))\n",
        "        return int(score) if score else 0\n",
        "    except Exception as e:\n",
        "        print(f\"Error judging similarity: {e}\")\n",
        "        return 0  # Fallback to minimum score\n",
        "\n",
        "def calculate_character_accuracy(target, proposal):\n",
        "    \"\"\"\n",
        "    Calculate character-level accuracy between target and proposal.\n",
        "    \n",
        "    Args:\n",
        "        target: The ground truth utterance\n",
        "        proposal: The generated proposal\n",
        "    \n",
        "    Returns:\n",
        "        Character accuracy score between 0 and 1\n",
        "    \"\"\"\n",
        "    if not target or not proposal:\n",
        "        return 0.0\n",
        "    \n",
        "    # Use Levenshtein distance approximated by sequence matcher\n",
        "    import difflib\n",
        "    similarity = difflib.SequenceMatcher(None, target, proposal).ratio()\n",
        "    return similarity\n",
        "\n",
        "def calculate_word_accuracy(target, proposal):\n",
        "    \"\"\"\n",
        "    Calculate word-level accuracy between target and proposal.\n",
        "    \n",
        "    Args:\n",
        "        target: The ground truth utterance\n",
        "        proposal: The generated proposal\n",
        "    \n",
        "    Returns:\n",
        "        Word accuracy score between 0 and 1\n",
        "    \"\"\"\n",
        "    if not target or not proposal:\n",
        "        return 0.0\n",
        "    \n",
        "    target_words = set(target.lower().split())\n",
        "    proposal_words = set(proposal.lower().split())\n",
        "    \n",
        "    if not target_words:\n",
        "        return 0.0\n",
        "    \n",
        "    intersection = target_words.intersection(proposal_words)\n",
        "    return len(intersection) / len(target_words)\n",
        "\n",
        "# Test the evaluation metrics\n",
        "target_text = \"I need to adjust my neck brace\"\n",
        "proposal_text = \"Need to adjust neck brace\"\n",
        "bad_proposal = \"I want to watch TV\"\n",
        "\n",
        "print(f\"Testing evaluation metrics with:\\n  Target: '{target_text}'\\n  Proposal: '{proposal_text}'\\n  Bad Proposal: '{bad_proposal}'\\n\")\n",
        "\n",
        "print(f\"Embedding Similarity (good): {calculate_embedding_similarity(target_text, proposal_text):.3f}\")\n",
        "print(f\"Embedding Similarity (bad): {calculate_embedding_similarity(target_text, bad_proposal):.3f}\\n\")\n",
        "\n",
        "print(f\"LLM Judge Score (good): {judge_similarity(target_text, proposal_text)}\")\n",
        "print(f\"LLM Judge Score (bad): {judge_similarity(target_text, bad_proposal)}\\n\")\n",
        "\n",
        "print(f\"Character Accuracy (good): {calculate_character_accuracy(target_text, proposal_text):.3f}\")\n",
        "print(f\"Character Accuracy (bad): {calculate_character_accuracy(target_text, bad_proposal):.3f}\\n\")\n",
        "\n",
        "print(f\"Word Accuracy (good): {calculate_word_accuracy(target_text, proposal_text):.3f}\")\n",
        "print(f\"Word Accuracy (bad): {calculate_word_accuracy(target_text, bad_proposal):.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Evaluation Framework"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_evaluation(corpus_df, test_df, partial_methods, generation_methods, \n",
        "                    evaluation_metrics, sample_size=None):\n",
        "    \"\"\"\n",
        "    Run a comprehensive evaluation of different partial utterance methods,\n",
        "    generation methods, and evaluation metrics.\n",
        "    \n",
        "    Args:\n",
        "        corpus_df: DataFrame with corpus utterances\n",
        "        test_df: DataFrame with test utterances\n",
        "        partial_methods: List of functions to create partial utterances\n",
        "        generation_methods: Dictionary of generation method functions\n",
        "        evaluation_metrics: Dictionary of evaluation metric functions\n",
        "        sample_size: Number of test examples to evaluate (None for all)\n",
        "    \n",
        "    Returns:\n",
        "        DataFrame with evaluation results\n",
        "    \"\"\"\n",
        "    # Sample test data if requested\n",
        "    if sample_size and sample_size < len(test_df):\n",
        "        eval_df = test_df.sample(sample_size, random_state=42).reset_index(drop=True)\n",
        "    else:\n",
        "        eval_df = test_df\n",
        "    \n",
        "    results = []\n",
        "    \n",
        "    for i, row in eval_df.iterrows():\n",
        "        target_text = row['content']\n",
        "        \n",
        "        # Skip very short utterances\n",
        "        if len(target_text.split()) < 3:\n",
        "            continue\n",
        "        \n",
        "        print(f\"Processing example {i+1}/{len(eval_df)}: '{target_text[:30]}...'\")\n",
        "        \n",
        "        # Extract context information\n",
        "        timestamp = row['timestamp']\n",
        "        latitude = row['latitude']\n",
        "        longitude = row['longitude']\n",
        "        context = f\"Time: {timestamp}, Location: {latitude}, {longitude}\"\n",
        "        \n",
        "        # Test each partial utterance method\n",
        "        for partial_method_name, partial_method in partial_methods.items():\n",
        "            partial_text = partial_method(target_text)\n",
        "            \n",
        "            # Skip if partial text is the same as target\n",
        "            if partial_text == target_text:\n",
        "                continue\n",
        "            \n",
        "            # Test each generation method\n",
        "            for gen_method_name, gen_method in generation_methods.items():\n",
        "                try:\n",
        "                    # Generate proposal\n",
        "                    if gen_method_name == \"context_only\":\n",
        "                        proposal = gen_method(partial_text, context)\n",
        "                    else:\n",
        "                        proposal = gen_method(corpus_df, partial_text, context)\n",
        "                    \n",
        "                    # Evaluate with each metric\n",
        "                    result = {\n",
        "                        'target': target_text,\n",
        "                        'partial': partial_text,\n",
        "                        'proposal': proposal,\n",
        "                        'partial_method': partial_method_name,\n",
        "                        'generation_method': gen_method_name,\n",
        "                    }\n",
        "                    \n",
        "                    for metric_name, metric_func in evaluation_metrics.items():\n",
        "                        result[metric_name] = metric_func(target_text, proposal)\n",
        "                    \n",
        "                    results.append(result)\n",
        "                    \n",
        "                except Exception as e:\n",
        "                    print(f\"  Error with {partial_method_name} + {gen_method_name}: {e}\")\n",
        "                    \n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "# Define the methods to evaluate\n",
        "partial_methods = {\n",
        "    'prefix_3': lambda text: create_prefix_partial(text, 3),\n",
        "    'prefix_2': lambda text: create_prefix_partial(text, 2),\n",
        "    'keyword_2': lambda text: create_keyword_partial(text, 2),\n",
        "    'random': lambda text: create_random_partial(text)\n",
        "}\n",
        "\n",
        "generation_methods = {\n",
        "    'lexical': generate_with_lexical_retrieval,\n",
        "    'tfidf': generate_with_tfidf_retrieval,\n",
        "    'embedding': generate_with_embedding_retrieval,\n",
        "    'context_only': generate_with_context_only\n",
        "}\n",
        "\n",
        "evaluation_metrics = {\n",
        "    'embedding_similarity': calculate_embedding_similarity,\n",
        "    'llm_judge_score': judge_similarity,\n",
        "    'character_accuracy': calculate_character_accuracy,\n",
        "    'word_accuracy': calculate_word_accuracy\n",
        "}\n",
        "\n",
        "print(\"Evaluation framework configured.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Run the Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run the evaluation on a subset of test data\n",
        "# Set sample_size to None to evaluate all test data (may take a long time)\n",
        "SAMPLE_SIZE = 10  # Start with a small sample for testing\n",
        "\n",
        "# Run the evaluation\n",
        "results_df = run_evaluation(\n",
        "    corpus_df, test_df, \n",
        "    partial_methods, generation_methods, evaluation_metrics,\n",
        "    sample_size=SAMPLE_SIZE\n",
        ")\n",
        "\n",
        "print(f\"\\nEvaluation complete. Generated {len(results_df)} results.\")\n",
        "results_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Analyze Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Aggregate results by partial method and generation method\n",
        "if not results_df.empty:\n",
        "    # Group by methods and calculate mean scores\n",
        "    grouped_results = results_df.groupby(['partial_method', 'generation_method']).agg({\n",
        "        'embedding_similarity': 'mean',\n",
        "        'llm_judge_score': 'mean',\n",
        "        'character_accuracy': 'mean',\n",
        "        'word_accuracy': 'mean',\n",
        "        'target': 'count'  # Count of samples\n",
        "    }).rename(columns={'target': 'count'}).reset_index()\n",
        "    \n",
        "    # Display the results\n",
        "    display(grouped_results)\n",
        "    \n",
        "    # Visualize the results\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "    fig.suptitle('Performance by Partial Method and Generation Method', fontsize=16)\n",
        "    \n",
        "    metrics = ['embedding_similarity', 'llm_judge_score', 'character_accuracy', 'word_accuracy']\n",
        "    titles = ['Embedding Similarity', 'LLM Judge Score', 'Character Accuracy', 'Word Accuracy']\n",
        "    \n",
        "    for ax, metric, title in zip(axes.flatten(), metrics, titles):\n",
        "        pivot = grouped_results.pivot(index='partial_method', columns='generation_method', values=metric)\n",
        "        sns.heatmap(pivot, annot=True, cmap='YlGnBu', fmt='.3f', ax=ax)\n",
        "        ax.set_title(title)\n",
        "        \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "else:\n",
        "    print(\"No results to analyze. Please run the evaluation first.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze the best performing combinations\n",
        "if not results_df.empty:\n",
        "    # Find the best combination for each metric\n",
        "    best_combinations = {}\n",
        "    \n",
        "    for metric in metrics:\n",
        "        best_idx = results_df[metric].idxmax()\n",
        "        best_combination = results_df.loc[best_idx, ['partial_method', 'generation_method', metric]]\n",
        "        best_combinations[metric] = best_combination\n",
        "    \n",
        "    print(\"Best performing combinations:\")\n",
        "    for metric, combination in best_combinations.items():\n",
        "        print(f\"  {metric}: {combination['partial_method']} + {combination['generation_method']} (score: {combination[metric]:.3f})\")\n",
        "    \n",
        "    # Show example outputs for the best combination\n",
        "    best_metric = max(metrics, key=lambda m: best_combinations[m].iloc[2])\n",
        "    best_partial = best_combinations[best_metric]['partial_method']\n",
        "    best_generation = best_combinations[best_metric]['generation_method']\n",
        "    \n",
        "    print(f\"\\nExamples for best combination ({best_partial} + {best_generation}):\")\n",
        "    examples = results_df[(results_df['partial_method'] == best_partial) & \n",
        "                           (results_df['generation_method'] == best_generation)].head(5)\n",
        "    \n",
        "    for _, example in examples.iterrows():\n",
        "        print(f\"  Partial: '{example['partial']}'\")\n",
        "        print(f\"  Target:  '{example['target']}'\")\n",
        "        print(f\"  Proposal: '{example['proposal']}'\")\n",
        "        print(f\"  Scores: Emb={example['embedding_similarity']:.3f}, LLM={example['llm_judge_score']:.1f}\\n\")\n",
        "else:\n",
        "    print(\"No results to analyze. Please run the evaluation first.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Error Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze common error patterns\n",
        "if not results_df.empty:\n",
        "    # Find examples with low similarity scores\n",
        "    low_similarity = results_df[results_df['embedding_similarity'] < 0.3].sort_values('embedding_similarity')\n",
        "    \n",
        "    print(\"Examples with low embedding similarity:\")\n",
        "    for _, example in low_similarity.head(5).iterrows():\n",
        "        print(f\"  Partial: '{example['partial']}'\")\n",
        "        print(f\"  Target:  '{example['target']}'\")\n",
        "        print(f\"  Proposal: '{example['proposal']}'\")\n",
        "        print(f\"  Method: {example['partial_method']} + {example['generation_method']}\")\n",
        "        print(f\"  Score: {example['embedding_similarity']:.3f}\\n\")\n",
        "    \n",
        "    # Analyze partial method performance\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "    \n",
        "    # Compare partial methods\n",
        "    partial_perf = results_df.groupby('partial_method')[['embedding_similarity', 'llm_judge_score']].mean()\n",
        "    partial_perf.plot(kind='bar', ax=axes[0])\n",
        "    axes[0].set_title('Performance by Partial Method')\n",
        "    axes[0].set_ylabel('Score')\n",
        "    axes[0].legend(['Embedding Similarity', 'LLM Judge Score'])\n",
        "    \n",
        "    # Compare generation methods\n",
        "    gen_perf = results_df.groupby('generation_method')[['embedding_similarity', 'llm_judge_score']].mean()\n",
        "    gen_perf.plot(kind='bar', ax=axes[1])\n",
        "    axes[1].set_title('Performance by Generation Method')\n",
        "    axes[1].set_ylabel('Score')\n",
        "    axes[1].legend(['Embedding Similarity', 'LLM Judge Score'])\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"No results to analyze. Please run the evaluation first.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Temporal Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze performance across different times of day\n",
        "if not results_df.empty:\n",
        "    # Add time information to results\n",
        "    enriched_results = []\n",
        "    \n",
        "    for _, row in results_df.iterrows():\n",
        "        # Find the corresponding test row to get time information\n",
        "        test_row = test_df[test_df['content'] == row['target']].iloc[0] if not test_df[test_df['content'] == row['target']].empty else None\n",
        "        \n",
        "        if test_row is not None:\n",
        "            enriched_row = row.copy()\n",
        "            enriched_row['hour'] = test_row['hour']\n",
        "            enriched_results.append(enriched_row)\n",
        "    \n",
        "    if enriched_results:\n",
        "        enriched_df = pd.DataFrame(enriched_results)\n",
        "        \n",
        "        # Group by time of day\n",
        "        time_performance = enriched_df.groupby('hour')[['embedding_similarity', 'llm_judge_score']].mean()\n",
        "        \n",
        "        # Plot performance by time of day\n",
        "        fig, ax = plt.subplots(figsize=(12, 5))\n",
        "        time_performance.plot(kind='line', ax=ax, marker='o')\n",
        "        ax.set_title('Performance by Time of Day')\n",
        "        ax.set_xlabel('Hour of Day')\n",
        "        ax.set_ylabel('Score')\n",
        "        ax.legend(['Embedding Similarity', 'LLM Judge Score'])\n",
        "        ax.set_xticks(range(0, 24))\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        \n",
        "        # Analyze by time periods (morning, afternoon, evening, night)\n",
        "        def get_time_period(hour):\n",
        "            if 5 <= hour < 12:\n",
        "                return 'Morning'\n",
        "            elif 12 <= hour < 17:\n",
        "                return 'Afternoon'\n",
        "            elif 17 <= hour < 22:\n",
        "                return 'Evening'\n",
        "            else:\n",
        "                return 'Night'\n",
        "        \n",
        "        enriched_df['time_period'] = enriched_df['hour'].apply(get_time_period)\n",
        "        period_performance = enriched_df.groupby('time_period')[['embedding_similarity', 'llm_judge_score']].mean()\n",
        "        \n",
        "        # Plot performance by time period\n",
        "        fig, ax = plt.subplots(figsize=(10, 5))\n",
        "        period_performance.plot(kind='bar', ax=ax)\n",
        "        ax.set_title('Performance by Time Period')\n",
        "        ax.set_ylabel('Score')\n",
        "        ax.legend(['Embedding Similarity', 'LLM Judge Score'])\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(\"Could not match results with test data for temporal analysis.\")\n",
        "else:\n",
        "    print(\"No results to analyze. Please run the evaluation first.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13. Conclusions and Next Steps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Key Findings\n",
        "\n",
        "Based on the evaluation results, we can draw the following conclusions:\n",
        "\n",
        "1. **Partial Utterance Methods:**\n",
        "   - Prefix truncation with 2-3 words seems to work well for preserving user intent\n",
        "   - Keyword extraction can be effective for focusing on core concepts\n",
        "   - Random selection of words performs poorly as expected\n",
        "\n",
        "2. **Generation Methods:**\n",
        "   - Retrieval-based approaches (lexical, TF-IDF, embedding) generally outperform context-only approaches\n",
        "   - Embedding-based retrieval shows promise for capturing semantic similarity\n",
        "   - Context-only generation may be useful when no relevant corpus examples are available\n",
        "\n",
        "3. **Evaluation Metrics:**\n",
        "   - Embedding similarity and LLM-judge scores provide complementary perspectives\n",
        "   - Character and word accuracy are stricter measures that may undervalue semantically similar but different phrasing\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "1. **Extend the Evaluation:**\n",
        "   - Test with larger sample sizes\n",
        "   - Incorporate more diverse partial utterance methods\n",
        "   - Implement additional retrieval methods (e.g., time-based filtering)\n",
        "\n",
        "2. **Hybrid Approaches:**\n",
        "   - Combine multiple retrieval methods\n",
        "   - Weight retrieved examples by relevance\n",
        "   - Implement contextual filtering based on time and location\n",
        "\n",
        "3. **Personalization:**\n",
        "   - Analyze performance on different types of utterances (needs, preferences, etc.)\n",
        "   - Develop adaptive partial utterance methods based on user behavior\n",
        "   - Implement temporal patterns for better context awareness\n",
        "\n",
        "4. **Real-World Testing:**\n",
        "   - Integrate with existing AAC systems\n",
        "   - Conduct user studies with actual AAC users\n",
        "   - Measure impact on communication speed and effort"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
