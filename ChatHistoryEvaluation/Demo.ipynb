{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Chat History-Driven LLM Evaluation Framework - Demo\n",
        "\n",
        "**Objective:** This notebook demonstrates how to use the Chat History-Driven LLM Evaluation Framework to evaluate different text completion methods for AAC users.\n",
        "\n",
        "## Overview\n",
        "\n",
        "The framework provides a systematic approach to compare:\n",
        "1. **Partial Utterance Methods** - How to create incomplete input from complete utterances\n",
        "2. **Proposal Generation Methods** - How to generate completions based on the partial input\n",
        "3. **Evaluation Metrics** - How to measure the quality of generated proposals\n",
        "\n",
        "## Data\n",
        "\n",
        "This demo uses real user chat history data from a person using an AAC system.\n",
        "**Important:** The data contains real user speech history and should not be shared or committed to version control."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies (run once)\n",
        "# !pip install -r requirements.txt\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Add utils directory to path\n",
        "sys.path.append(os.path.join(os.getcwd(), 'utils'))\n",
        "from utils.evaluation_utils import ChatHistoryEvaluator\n",
        "\n",
        "print(\"✅ Dependencies imported.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Initialize Evaluator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize evaluator with chat data\n",
        "chat_data_path = \"data/test/baton-export-2025-11-24-nofullstop.json\"\n",
        "\n",
        "# Note: If you haven't set your API key as an environment variable,\n",
        "# you'll be prompted to enter it\n",
        "evaluator = ChatHistoryEvaluator(\n",
        "    chat_data_path=chat_data_path,\n",
        "    corpus_ratio=0.67  # Use 2/3 of data for corpus\n",
        ")\n",
        "\n",
        "print(f\"✅ Evaluator initialized.\")\n",
        "print(f\"Corpus size: {len(evaluator.corpus_df)} utterances\")\n",
        "print(f\"Test size: {len(evaluator.test_df)} utterances\")\n",
        "print(f\"Date range: {evaluator.chat_df['timestamp'].min()} to {evaluator.chat_df['timestamp'].max()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Define Evaluation Methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define partial utterance methods\n",
        "partial_methods = {\n",
        "    'prefix_3': lambda text: evaluator.create_prefix_partial(text, 3),  # First 3 words\n",
        "    'prefix_2': lambda text: evaluator.create_prefix_partial(text, 2),  # First 2 words\n",
        "    'keyword_2': lambda text: evaluator.create_keyword_partial(text, 2),  # 2 most salient keywords\n",
        "}\n",
        "\n",
        "# Define generation methods\n",
        "generation_methods = {\n",
        "    'lexical': evaluator.generate_with_lexical_retrieval,      # Exact word matching\n",
        "    'tfidf': evaluator.generate_with_tfidf_retrieval,        # TF-IDF similarity\n",
        "    'embedding': evaluator.generate_with_embedding_retrieval,   # Semantic similarity\n",
        "}\n",
        "\n",
        "# Define evaluation metrics\n",
        "evaluation_metrics = {\n",
        "    'embedding_similarity': evaluator.calculate_embedding_similarity,  # Semantic similarity\n",
        "    'llm_judge_score': evaluator.judge_similarity,            # LLM evaluation\n",
        "    'character_accuracy': evaluator.calculate_character_accuracy, # Character match\n",
        "    'word_accuracy': evaluator.calculate_word_accuracy,        # Word overlap\n",
        "}\n",
        "\n",
        "print(\"✅ Evaluation methods defined.\")\n",
        "print(f\"Partial methods: {list(partial_methods.keys())}\")\n",
        "print(f\"Generation methods: {list(generation_methods.keys())}\")\n",
        "print(f\"Evaluation metrics: {list(evaluation_metrics.keys())}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Test Individual Methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Select a sample test utterance\n",
        "sample_utterance = evaluator.test_df.iloc[10]['content']\n",
        "print(f\"Sample utterance: '{sample_utterance}'\\n\")\n",
        "\n",
        "# Test partial utterance methods\n",
        "print(\"Partial utterance methods:\")\n",
        "for name, method in partial_methods.items():\n",
        "    partial = method(sample_utterance)\n",
        "    print(f\"  {name}: '{partial}'\")\n",
        "\n",
        "print(\"\\nTesting generation methods with prefix_3 partial:\")\n",
        "prefix_3_partial = partial_methods['prefix_3'](sample_utterance)\n",
        "context = f\"Time: {evaluator.test_df.iloc[10]['timestamp']}\"\n",
        "\n",
        "for name, method in generation_methods.items():\n",
        "    try:\n",
        "        proposal = method(evaluator, prefix_3_partial, context)\n",
        "        print(f\"  {name}: '{proposal}'\")\n",
        "    except Exception as e:\n",
        "        print(f\"  {name}: Error - {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Run Full Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run evaluation on a small sample\n",
        "SAMPLE_SIZE = 10  # Use a small sample for demo\n",
        "\n",
        "print(f\"Running evaluation on {SAMPLE_SIZE} test utterances...\")\n",
        "results_df = evaluator.run_evaluation(\n",
        "    partial_methods=partial_methods,\n",
        "    generation_methods=generation_methods,\n",
        "    evaluation_metrics=evaluation_metrics,\n",
        "    sample_size=SAMPLE_SIZE\n",
        ")\n",
        "\n",
        "print(f\"\\nEvaluation complete. Generated {len(results_df)} results.\")\n",
        "results_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Analyze Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Group by methods and calculate mean scores\n",
        "grouped_results = results_df.groupby(['partial_method', 'generation_method']).agg({\n",
        "    'embedding_similarity': 'mean',\n",
        "    'llm_judge_score': 'mean',\n",
        "    'character_accuracy': 'mean',\n",
        "    'word_accuracy': 'mean',\n",
        "    'target': 'count'  # Count of samples\n",
        "}).rename(columns={'target': 'count'}).reset_index()\n",
        "\n",
        "display(grouped_results)\n",
        "\n",
        "# Find best performing combination for each metric\n",
        "best_combinations = {}\n",
        "metrics = ['embedding_similarity', 'llm_judge_score', 'character_accuracy', 'word_accuracy']\n",
        "\n",
        "for metric in metrics:\n",
        "    best_idx = results_df[metric].idxmax()\n",
        "    best_combination = results_df.loc[best_idx, ['partial_method', 'generation_method', metric]]\n",
        "    best_combinations[metric] = best_combination\n",
        "\n",
        "print(\"\\nBest performing combinations:\")\n",
        "for metric, combination in best_combinations.items():\n",
        "    print(f\"  {metric}: {combination['partial_method']} + {combination['generation_method']} (score: {combination[metric]:.3f})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Visualize Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate visualizations\n",
        "evaluator.visualize_results(results_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Save Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save detailed results\n",
        "output_path = \"demo_results.csv\"\n",
        "results_df.to_csv(output_path, index=False)\n",
        "print(f\"Results saved to: {output_path}\")\n",
        "\n",
        "# Save summary statistics\n",
        "summary_path = \"demo_summary.csv\"\n",
        "grouped_results.to_csv(summary_path, index=False)\n",
        "print(f\"Summary saved to: {summary_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Based on this evaluation, we can observe:\n",
        "\n",
        "1. **Partial Utterance Methods:**\n",
        "   - Prefix truncation (especially 2-3 words) tends to preserve intent well\n",
        "   - Keyword extraction can focus on core concepts\n",
        "\n",
        "2. **Generation Methods:**\n",
        "   - Retrieval-based approaches generally outperform context-only\n",
        "   - Embedding-based retrieval shows promise for semantic similarity\n",
        "\n",
        "3. **Evaluation Metrics:**\n",
        "   - Different metrics highlight different aspects of performance\n",
        "   - Embedding similarity and LLM judge scores align well\n",
        "\n",
        "### Next Steps:\n",
        "\n",
        "1. Run evaluation on larger sample sizes for more robust results\n",
        "2. Experiment with hybrid approaches combining multiple retrieval methods\n",
        "3. Implement temporal filtering for time-based context\n",
        "4. Extend to location-based context filtering\n",
        "5. Compare different embedding models for better semantic matching"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
