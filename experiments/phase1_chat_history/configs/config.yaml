# Configuration for Chat History-Driven LLM Evaluation

# Data Configuration
data:
  chat_data_path: "data/real/chat_history/raw/baton-export-2025-11-24-nofullstop.json"
  corpus_ratio: 0.67 # Fraction of data to use as corpus (default: 2/3)

# Model Configuration
model:
  provider: "gemini" # LLM provider to use
  name: "gemini-2.0-flash-exp" # LLM model to use
  temperature: 0.2 # Temperature for generation
  embedding_model: "all-MiniLM-L6-v2" # Embedding model for similarity

# Evaluation Parameters
evaluation:
  sample_size: 10 # Number of test examples to evaluate (null for all)
  partial_utterance_methods:
    prefix_3:
      type: "prefix"
      n_words: 3
    prefix_2:
      type: "prefix"
      n_words: 2
    keyword_2:
      type: "keyword"
      n_keywords: 2
    random:
      type: "random"
      min_words: 1
      max_words: 3
  generation_methods:
    lexical:
      type: "lexical"
      top_k: 3
    tfidf:
      type: "tfidf"
      top_k: 3
    embedding:
      type: "embedding"
      top_k: 3
    context_only:
      type: "context_only"
  evaluation_metrics:
    embedding_similarity:
      enabled: true
    llm_judge_score:
      enabled: true
    character_accuracy:
      enabled: true
    word_accuracy:
      enabled: true

# Output Configuration
output:
  results_dir: "results"
  save_plots: true
  plot_format: "png" # png, pdf, svg
  plot_dpi: 300

# Advanced Options
advanced:
  temporal_analysis: true # Analyze performance by time of day
  error_analysis: true # Generate error analysis
  cache_embeddings: true # Cache embeddings for faster retrieval
