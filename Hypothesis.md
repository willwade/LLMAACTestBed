| Hypothesis | How to test | Blocked by | Result (current) |
| --- | --- | --- | --- |
| Adding personal context into an LLM through a series of questions improves response quality | Phase 2 synthetic context runs with profile + temporal + location prompts (e.g., `uv run run-phase2 --experiment all --provider openai --model gpt-4o-mini`). Compare LLM-judge scores with and without injected profile/story context. | None for synthetic; real profiles still pending consent. | Early synthetic runs: profile/story context improved judged similarity on telegraphic inputs; time/location alone gave minimal lift (consistent with AGENTS.md notes). Needs larger sweep and real profiles to quantify. |
| Adding chat history details improves response quality | Phase 1 chat history evaluation on real chat logs (331 utterances) or synthetic fallback: hold out last third as targets, feed first two-thirds as context; score with embedding similarity + LLM judge. | Real chat data availability on each user machine; currently using minimal pilot (`results/phase1_minimal_test.json`). | Pilot with 5-message context using GPT-4 showed 1.0 word accuracy and similarity 30 on a sample completion; wider run pending. Evaluation pipeline in `experiments/phase1_chat_history/` ready. |
| Adding chat history and personal context together | Phase 3 combined run: merge chat history with profile/social graph (`uv run run-phase3 --provider openai --model gpt-4o-mini`). Compare baseline vs enriched scores per chat. | Robust scored dataset (real chats + profiles) and cleaner scoring function. | Initial synthetic/profile-enhanced run on 37 chats showed mean improvement 0.0 (see `results/phase3_profile_enhanced_20251215_180747.json`), suggesting prompt design or scoring needs refinement. |
| Having the other half of the conversation improves output | Build paired-dialogue synthetic set (speaker A/B turns) and test completion with/without interlocutor turns; evaluate via LLM judge. | Labeled partner turns in real data; synthetic paired dataset not yet generated. | Not tested. |
| Labelled chat history (who you are talking to) helps output | Extend transcripts to include interlocutor IDs; run context filter with social graph before generation. | Lack of labeled interlocutor metadata in current datasets. | Not tested. |
| Label the first 2/3 of chat, then see if final 1/3 becomes self-labelled | Semi-supervised experiment: train/condition on labeled first segment, test auto-label accuracy on remainder. | Need labeled ground truth chat segments; pipeline to auto-evaluate labels. | Not tested. |
| How to get labelled data (AAC user labels or RSSI/BLE) | Data collection study: lightweight tagging UI + proximity signals to capture interlocutor identity alongside chats. | Participant recruitment and hardware logging setup. | Not started. |
| Having voice data from the other user helps | Multimodal experiment: add ASR of partner speech as context; measure effect on completion accuracy. | Audio capture/ASR pipeline; consented datasets. | Not tested. |
